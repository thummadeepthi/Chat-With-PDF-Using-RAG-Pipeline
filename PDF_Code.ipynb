{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "def create_embeddings(chunks):\n",
    "    embeddings = [embedding_model.embed_query(chunk) for chunk in chunks]\n",
    "    return embeddings\n",
    "\n",
    "def store_embeddings_in_faiss(embeddings, chunks):\n",
    "    vector_db = FAISS(embeddings, chunks)\n",
    "    return vector_db\n",
    "\n",
    "def similarity_search(query, vector_db):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = vector_db.similarity_search(query_embedding, k=5)\n",
    "    return results\n",
    "\n",
    "def initialize_llm():\n",
    "    llm = OpenAI(model_name=\"gpt-4\")\n",
    "    return llm\n",
    "\n",
    "def generate_response(llm, query, retrieved_chunks):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"retrieved_chunks\"],\n",
    "        template=\"\"\"\n",
    "        User Query: {query}\n",
    "        Relevant Information: {retrieved_chunks}\n",
    "        Answer the query based on the above context with exact details.\n",
    "        \"\"\"\n",
    "    )\n",
    "    context = ' '.join(retrieved_chunks)\n",
    "    prompt = prompt_template.format(query=query, retrieved_chunks=context)\n",
    "    response = llm(prompt)\n",
    "    return response\n",
    "\n",
    "def main():\n",
    "    pdf_paths = input(\"Enter the paths of PDF files, separated by commas: \").split(',')\n",
    "    user_query = input(\"Enter your query: \")\n",
    "\n",
    "    all_chunks = []\n",
    "    chunk_mapping = {}\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        text = extract_text_from_pdf(pdf_path.strip())\n",
    "        chunks = chunk_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "        chunk_mapping.update({chunk: pdf_path for chunk in chunks})\n",
    "\n",
    "    embeddings = create_embeddings(all_chunks)\n",
    "    vector_db = store_embeddings_in_faiss(embeddings, all_chunks)\n",
    "\n",
    "    retrieved_chunks = similarity_search(user_query, vector_db)\n",
    "    retrieved_texts = [chunk[1] for chunk in retrieved_chunks]\n",
    "  \n",
    "    llm = initialize_llm()\n",
    "    response = generate_response(llm, user_query, retrieved_texts)\n",
    "\n",
    "    print(\"Response:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
